{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install yfinance langchain_pinecone openai python-dotenv langchain-community sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from openai import OpenAI\n",
    "import dotenv\n",
    "import json\n",
    "import yfinance as yf\n",
    "import concurrent.futures\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# from google.colab import userdata\n",
    "from langchain.schema import Document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_info(symbol: str) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieves and formats detailed information about a stock from Yahoo Finance.\n",
    "\n",
    "    Args:\n",
    "        symbol (str): The stock ticker symbol to look up.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing detailed stock information, including ticker, name,\n",
    "              business summary, city, state, country, industry, and sector.\n",
    "    \"\"\"\n",
    "    # headers = {\n",
    "    # 'Authorization': f'Bearer YAHOO_ACCESS_TOKEN'\n",
    "    # }\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {os.getenv(\"YAHOO_ACCESS_TOKEN\")}'\n",
    "    }\n",
    "    session = requests.Session()\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    data = yf.Ticker(symbol, session=session)\n",
    "    # data = yf.Ticker(symbol)\n",
    "    stock_info = data.info\n",
    "\n",
    "    properties = {\n",
    "        \"Ticker\": stock_info.get('symbol', 'Information not available'),\n",
    "        'Name': stock_info.get('longName', 'Information not available'),\n",
    "        'Business Summary': stock_info.get('longBusinessSummary'),\n",
    "        'City': stock_info.get('city', 'Information not available'),\n",
    "        'State': stock_info.get('state', 'Information not available'),\n",
    "        'Country': stock_info.get('country', 'Information not available'),\n",
    "        'Industry': stock_info.get('industry', 'Information not available'),\n",
    "        'Sector': stock_info.get('sector', 'Information not available')\n",
    "    }\n",
    "\n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the two sentences: 0.6133\n"
     ]
    }
   ],
   "source": [
    "def get_huggingface_embeddings(text, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
    "    \"\"\"\n",
    "    Generates embeddings for the given text using a specified Hugging Face model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to generate embeddings for.\n",
    "        model_name (str): The name of the Hugging Face model to use.\n",
    "                          Defaults to \"sentence-transformers/all-mpnet-base-v2\".\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The generated embeddings as a NumPy array.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    return model.encode(text)\n",
    "\n",
    "\n",
    "def cosine_similarity_between_sentences(sentence1, sentence2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two sentences.\n",
    "\n",
    "    Args:\n",
    "        sentence1 (str): The first sentence for similarity comparison.\n",
    "        sentence2 (str): The second sentence for similarity comparison.\n",
    "\n",
    "    Returns:\n",
    "        float: The cosine similarity score between the two sentences,\n",
    "               ranging from -1 (completely opposite) to 1 (identical).\n",
    "\n",
    "    Notes:\n",
    "        Prints the similarity score to the console in a formatted string.\n",
    "    \"\"\"\n",
    "    # Get embeddings for both sentences\n",
    "    embedding1 = np.array(get_huggingface_embeddings(sentence1))\n",
    "    embedding2 = np.array(get_huggingface_embeddings(sentence2))\n",
    "\n",
    "    # Reshape embeddings for cosine_similarity function\n",
    "    embedding1 = embedding1.reshape(1, -1)\n",
    "    embedding2 = embedding2.reshape(1, -1)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(embedding1, embedding2)\n",
    "    similarity_score = similarity[0][0]\n",
    "    print(f\"Cosine similarity between the two sentences: {similarity_score:.4f}\")\n",
    "    return similarity_score\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sentence1 = \"I like walking to the park\"\n",
    "sentence2 = \"I like running to the office\"\n",
    "\n",
    "similarity = cosine_similarity_between_sentences(sentence1, sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully and saved as 'company_tickers.json'\n"
     ]
    }
   ],
   "source": [
    "def get_company_tickers():\n",
    "    \"\"\"\n",
    "    Downloads and parses the Stock ticker symbols from the GitHub-hosted SEC company tickers JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing company tickers and related information.\n",
    "\n",
    "    Notes:\n",
    "        The data is sourced from the official SEC website via a GitHub repository:\n",
    "        https://raw.githubusercontent.com/team-headstart/Financial-Analysis-and-Automation-with-LLMs/main/company_tickers.json\n",
    "    \"\"\"\n",
    "    # URL to fetch the raw JSON file from GitHub\n",
    "    url = \"https://raw.githubusercontent.com/team-headstart/Financial-Analysis-and-Automation-with-LLMs/main/company_tickers.json\"\n",
    "\n",
    "    # Making a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Checking if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON content directly\n",
    "        company_tickers = json.loads(response.content.decode('utf-8'))\n",
    "\n",
    "        # Optionally save the content to a local file for future use\n",
    "        with open(\"company_tickers.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(company_tickers, file, indent=4)\n",
    "\n",
    "        print(\"File downloaded successfully and saved as 'company_tickers.json'\")\n",
    "        return company_tickers\n",
    "    else:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "company_tickers = get_company_tickers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9998"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(company_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pg/wwfxmq4x1vx_t00m5pz1z4nr0000gp/T/ipykernel_13616/4218980554.py:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf_embeddings = HuggingFaceEmbeddings()\n",
      "/var/folders/pg/wwfxmq4x1vx_t00m5pz1z4nr0000gp/T/ipykernel_13616/4218980554.py:9: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  hf_embeddings = HuggingFaceEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "index_name = \"stocks\"\n",
    "namespace = \"stock-descriptions\"\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=hf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Processing\n",
    "# This is a slow process, so we will use a more efficient method later\n",
    "\n",
    "# for idx, stock in company_tickers.items():\n",
    "#     stock_ticker = stock['ticker']\n",
    "#     stock_data = get_stock_info(stock_ticker)\n",
    "#     stock_description = stock_data['Business Summary']\n",
    "\n",
    "#     print(f\"Processing stock {idx} / {len(company_tickers)} :\", stock_ticker)\n",
    "\n",
    "#     vectorstore_from_documents = PineconeVectorStore.from_documents(\n",
    "#         documents=[Document(page_content=stock_description, metadata=stock_data)],\n",
    "#         embedding=hf_embeddings,\n",
    "#         index_name=index_name,\n",
    "#         namespace=namespace\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10003 successful tickers\n",
      "Loaded 0 unsuccessful tickers\n"
     ]
    }
   ],
   "source": [
    "# This loads the existing successful tickers from the file\n",
    "# If you want to start over, delete the successful_tickers.txt file\n",
    "\n",
    "\n",
    "# Initialize tracking lists\n",
    "successful_tickers = []\n",
    "unsuccessful_tickers = []\n",
    "\n",
    "# Load existing successful/unsuccessful tickers\n",
    "try:\n",
    "    with open('successful_tickers.txt', 'r') as f:\n",
    "        successful_tickers = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"Loaded {len(successful_tickers)} successful tickers\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No existing successful tickers file found\")\n",
    "\n",
    "try:\n",
    "    with open('unsuccessful_tickers.txt', 'r') as f:\n",
    "        unsuccessful_tickers = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"Loaded {len(unsuccessful_tickers)} unsuccessful tickers\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No existing unsuccessful tickers file found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 8\n"
     ]
    }
   ],
   "source": [
    "# Check the number of CPU cores\n",
    "# This is important for parallel processing\n",
    "# Use all cores minus 1 as max_workers for parallel processing\n",
    "# The less cores you have, the more you should reduce the batch size\n",
    "# 35 is a good starting point for most machines with 8 cores -- 7 max_workers * 5 = batch size 35 for m1 macbook pros\n",
    "import os\n",
    "print(f\"Number of CPU cores: {os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Processing\n",
    "# This is a faster process, but it requires more memory\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def process_stock(stock_ticker: str) -> str:\n",
    "    # Skip if already processed\n",
    "    # time.sleep(.5)\n",
    "    if stock_ticker in successful_tickers:\n",
    "        return f\"Already processed {stock_ticker}\"\n",
    "\n",
    "    try:\n",
    "        # Get and store stock data\n",
    "        stock_data = get_stock_info(stock_ticker)\n",
    "        if stock_data['Business Summary'] is None:\n",
    "            stock_data['Business Summary'] = \"No business summary available\"\n",
    "        stock_description = stock_data['Business Summary']\n",
    "\n",
    "        # stock_description = stock_data['Business Summary'] or \"No business summary available\"\n",
    "        # print(stock_description)\n",
    "\n",
    "        # if stock_description is None:\n",
    "        #     return f\"Skipping {stock_ticker}: No business summary available\"\n",
    "\n",
    "        # Store stock description in Pinecone\n",
    "        vectorstore_from_texts = PineconeVectorStore.from_documents(\n",
    "            documents=[Document(page_content=stock_description, metadata=stock_data)],\n",
    "            embedding=hf_embeddings,\n",
    "            index_name=index_name,\n",
    "            namespace=namespace\n",
    "        )\n",
    "\n",
    "        # Track success\n",
    "        with open('successful_tickers.txt', 'a') as f:\n",
    "            f.write(f\"{stock_ticker}\\n\")\n",
    "        successful_tickers.append(stock_ticker)\n",
    "\n",
    "        return f\"Processed {stock_ticker} successfully\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Track failure\n",
    "        with open('unsuccessful_tickers.txt', 'a') as f:\n",
    "            f.write(f\"{stock_ticker}\\n\")\n",
    "        unsuccessful_tickers.append(stock_ticker)\n",
    "\n",
    "        return f\"ERROR processing {stock_ticker}: {e}\"\n",
    "\n",
    "def parallel_process_stocks(tickers: list, batch_size=35, max_workers: int = 5) -> None:\n",
    "    for i in range(0, len(tickers), batch_size):\n",
    "        print(f\"\\nProcessing batch {i//batch_size + 1}\")\n",
    "        batch = tickers[i:i + batch_size]\n",
    "\n",
    "        # with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        #     future_to_ticker = {\n",
    "        #     executor.submit(process_stock, ticker): ticker\n",
    "        #     for ticker in tickers\n",
    "        # }\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_ticker = {\n",
    "            executor.submit(process_stock, ticker): ticker\n",
    "            for ticker in batch\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_ticker):\n",
    "            ticker = future_to_ticker[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                print(result)\n",
    "\n",
    "                # Stop on error\n",
    "            #     if result.startswith(\"ERROR\"):\n",
    "            #         print(f\"Stopping program due to error in {ticker}\")\n",
    "            #         executor.shutdown(wait=False)\n",
    "            #         raise SystemExit(1)\n",
    "\n",
    "            # except Exception as exc:\n",
    "            #     print(f'{ticker} generated an exception: {exc}')\n",
    "            #     print(\"Stopping program due to exception\")\n",
    "            #     executor.shutdown(wait=True)\n",
    "            #     raise SystemExit(1)\n",
    "            except Exception as exc:\n",
    "                print(f'ERROR processing {ticker}: {exc}')\n",
    "                continue\n",
    "\n",
    "# Prepare your tickers\n",
    "tickers_to_process = [company_tickers[num]['ticker'] for num in company_tickers.keys()]\n",
    "\n",
    "# Process them\n",
    "parallel_process_stocks(tickers_to_process, max_workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10003 successful tickers\n",
      "Loaded 0 unsuccessful tickers\n"
     ]
    }
   ],
   "source": [
    "# Remove tickers that were successfully processed from the unsuccessful_tickers.txt file\n",
    "for ticker in unsuccessful_tickers:\n",
    "  if ticker in successful_tickers:\n",
    "    print(f\"Removing {ticker} from unsuccessful_tickers.txt\")\n",
    "    with open('unsuccessful_tickers.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    with open('unsuccessful_tickers.txt', 'w') as f:\n",
    "        for line in lines:\n",
    "            if line.strip() != ticker:\n",
    "                f.write(line)\n",
    "\n",
    "successful_tickers = []\n",
    "unsuccessful_tickers = []\n",
    "\n",
    "# Load existing successful/unsuccessful tickers\n",
    "try:\n",
    "    with open('successful_tickers.txt', 'r') as f:\n",
    "        successful_tickers = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"Loaded {len(successful_tickers)} successful tickers\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No existing successful tickers file found\")\n",
    "\n",
    "try:\n",
    "    with open('unsuccessful_tickers.txt', 'r') as f:\n",
    "        unsuccessful_tickers = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"Loaded {len(unsuccessful_tickers)} unsuccessful tickers\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No existing unsuccessful tickers file found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Connect to your Pinecone index\n",
    "pinecone_index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What are some companies that manufacture consumer hardware?\"\n",
    "# query = \"Give me the Business Summary and Ticker for the company with the ticker symbol LDDD\"\n",
    "query = \"Give me the stock data for the company with the ticker symbol NTIC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_query_embedding = get_huggingface_embeddings(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_matches = pinecone_index.query(vector=raw_query_embedding.tolist(), top_k=10, include_metadata=True, namespace=namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = [item['metadata']['text'] for item in top_matches['matches']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_query = \"<CONTEXT>\\n\" + \"\\n\\n-------\\n\\n\".join(contexts[ : 10]) + \"\\n-------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(augmented_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://api.groq.com/openai/v1\",\n",
    "  api_key=groq_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"You are an expert at providing answers about stocks. Please answer my question provided.\n",
    "\"\"\"\n",
    "\n",
    "llm_response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": augmented_query}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = llm_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Northern Technologies International Corporation develops and markets rust and corrosion inhibiting solutions in North America, South America, Europe, Asia, the Middle East and internationally. It offers rust and corrosion inhibiting products, such as plastic and paper packaging, liquids, coatings, rust removers, cleaners, diffusers, and engineered solutions designed for the oil and gas industry under the ZERUST brand. The company provides a portfolio of bio-based and certified compostable polymer resin compounds and finished products under the Natur-Tec brand. In addition, it offers on-site and technical consulting for rust and corrosion prevention issues. It sells its products and services to automotive, electronics, electrical, mechanical, military, retail consumer, and oil and gas markets through direct sales force, network of independent distributors, agents, manufacturer's sales representatives, and strategic partners. The company was founded in 1970 and is headquartered in Circle Pines, Minnesota.\n"
     ]
    }
   ],
   "source": [
    "stock_data = get_stock_info(\"NTIC\")\n",
    "print(stock_data['Business Summary'])\n",
    "\n",
    "if stock_data['Business Summary'] is None:\n",
    "    stock_data['Business Summary'] = \"No business summary available\"\n",
    "stock_description = stock_data['Business Summary']\n",
    "\n",
    "vectorstore_from_texts = PineconeVectorStore.from_documents(\n",
    "            documents=[Document(page_content=stock_description, metadata=stock_data)],\n",
    "            embedding=hf_embeddings,\n",
    "            index_name=index_name,\n",
    "    namespace=namespace\n",
    ")\n",
    "# print(hf_embeddings)\n",
    "# print(namespace)\n",
    "# print(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_streamlit():\n",
    "  os.system(\"streamlit run app.py --server.port 8501\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from openai import OpenAI\n",
    "import dotenv\n",
    "import json\n",
    "import yfinance as yf\n",
    "import concurrent.futures\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# from google.colab import userdata\n",
    "from langchain.schema import Document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "index_name = \"stocks\"\n",
    "namespace = \"stock-descriptions\"\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=hf_embeddings)\n",
    "\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Connect to your Pinecone index\n",
    "pinecone_index = pc.Index(index_name)\n",
    "\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://api.groq.com/openai/v1\",\n",
    "  api_key=groq_api_key\n",
    ")\n",
    "\n",
    "def get_huggingface_embeddings(text, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    return model.encode(text)\n",
    "\n",
    "# query = \"apple\"\n",
    "\n",
    "# system_prompt = f\"\"\"You are an expert in the field of embeddings, cosine similarity search and vector databases. Given the following query for a vector database with a pinecone index that stock descriptions are stored in, please provide a better query that will return more relevant results:\n",
    "\n",
    "# Query: {query}\n",
    "# \"\"\"\n",
    "\n",
    "# llm_response = client.chat.completions.create(\n",
    "#     model=\"llama-3.1-70b-versatile\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": system_prompt},\n",
    "#         {\"role\": \"user\", \"content\": query}\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# response = llm_response.choices[0].message.content\n",
    "# print(\"RESPONSE\", response)\n",
    "\n",
    "\n",
    "# raw_query_embedding = get_huggingface_embeddings(query)\n",
    "\n",
    "# top_matches = pinecone_index.query(vector=raw_query_embedding.tolist(), top_k=10, include_metadata=True, namespace=namespace)\n",
    "\n",
    "# print(\"Checking matches for GOOGL:\")\n",
    "# for match in top_matches['matches']:\n",
    "#     ticker = match['metadata'].get('Ticker')\n",
    "#     score = match['score']\n",
    "#     print(f\"Ticker: {ticker}, Score: {score}\")\n",
    "#     if ticker == 'GOOGL':\n",
    "#         print(\"\\nFound GOOGL!\")\n",
    "#         print(\"Full metadata:\", match['metadata'])\n",
    "# print(\"top_matches\", top_matches)\n",
    "def HandleQuery(query):\n",
    "    raw_query_embedding = get_huggingface_embeddings(query)\n",
    "\n",
    "    top_matches = pinecone_index.query(vector=raw_query_embedding.tolist(), top_k=10, include_metadata=True, namespace=namespace)\n",
    "\n",
    "    contexts = [item['metadata']['text'] for item in top_matches['matches']]\n",
    "\n",
    "    augmented_query = \"<CONTEXT>\\n\" + \"\\n\\n-------\\n\\n\".join(contexts[ : 10]) + \"\\n-------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query\n",
    "\n",
    "    system_prompt = f\"\"\"You are an expert at providing answers about stocks. Please answer my question provided.\n",
    "\n",
    "    When giving your response, please do not mention the context provided to you or the query.\n",
    "\n",
    "    Please provide a detailed answer to the question.\n",
    "\n",
    "    Please provide all of the answers that you receive from the context provided.\n",
    "\n",
    "    Please provide the answers from most relevant to least relevant.\n",
    "\n",
    "    Please provide the answer in a markdown format.\n",
    "\n",
    "    Please be consistent in the markdown format for all of your answers.\n",
    "    \"\"\"\n",
    "\n",
    "    llm_response = client.chat.completions.create(\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": augmented_query}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response = llm_response.choices[0].message.content\n",
    "    return response\n",
    "\n",
    "# print(\"RESPONSE\", response)\n",
    "# print(\"something\")\n",
    "\n",
    "# query = \"Which stocks are in the consumer staples sector?\"\n",
    "\n",
    "# Title\n",
    "st.title('My Stock Analysis App')\n",
    "\n",
    "# Add a text input for stock symbol\n",
    "# st.write(\"### Response:\")\n",
    "query = st.text_input('Ask About Stocks:',)\n",
    "\n",
    "\n",
    "# Add a button\n",
    "if st.button('Get Stock Info'):\n",
    "    st.write(f'Getting info for {query}...')\n",
    "    response = HandleQuery(query)\n",
    "    \n",
    "    # Display the response\n",
    "    st.write(\"### Response:\")\n",
    "    st.write(response)\n",
    "    \n",
    "    # Optional: Add formatting\n",
    "    st.markdown(\"---\")  # Add a divider\n",
    "    \n",
    "    # You can also add expandable sections\n",
    "    # with st.expander(\"Show Raw Response\"):\n",
    "    #     st.code(response)  # Shows response in a code block\n",
    "        \n",
    "    # Add error handling\n",
    "    if not response:\n",
    "        st.error(\"No information found for this query.\")\n",
    "    \n",
    "# Add a sidebar\n",
    "# st.sidebar.title('Options')\n",
    "# show_charts = st.sidebar.checkbox('Show Charts')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
